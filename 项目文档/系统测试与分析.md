# 5 系统测试与分析 (Testing & Analysis)

本章是 AgentMarkWeb 项目报告中的“质量保证与验收支撑”章节，目标不是复述脚本运行过程，而是以项目书体裁说明：系统在真实类 Agent 运行链路中的可用性、隐蔽性与鲁棒性如何被验证、验证采用何种口径、证据如何留存、以及最终可据此完成验收与问题闭环。

本章内容严格以仓库的 README/代码/实验配置为依据，覆盖三类典型 Agent 场景：ToolBench（工具调用型 Agent）、ALFWorld（具身智能 Agent）以及 Oasis（社会模拟 Agent），并保持与 Agent Mark 原论文一致的评测目标：在尽量不改变任务效用的前提下，实现可验证的行为层水印嵌入与恢复，并能在复杂干扰下可靠解码。

---

## 5.1 章节定位与验收目标

### 5.1.1 质量目标与验收维度

围绕系统核心创新点——“Agent 行为层水印嵌入机制”，本项目将质量目标归纳为三个维度，并将其作为验收与评审的主线：

1) **可用性（Availability）**  
系统应能在真实 Agent 执行过程中稳定嵌入水印，并在事后基于同一概率分布与上下文信息完成恢复；在 ToolBench/ALFWorld/Oasis 等不同环境下均应具备可落地的嵌入与解码路径。

2) **隐蔽性（Imperceptibility）**  
水印机制应尽量保持行为分布“不可感知差异”，并在任务成功率、平均步骤数、平均耗时等效用指标上保持与 baseline 接近；当出现差异时，必须能通过日志证据定位原因（例如候选集变化、概率解析失败等）。

3) **纠错与鲁棒性（Error Resilience）**  
在日志缺失、轨迹截断、网络波动导致的步骤丢失等复杂条件下，系统应能通过纠删码与同步机制实现可恢复或可判定（例如 RLNC 满秩可解），并具备可量化的“恢复阈值/成功率/误报率”等指标。

### 5.1.2 验收判定方式（“通过/不通过”的落地口径）

项目书体裁通常要求明确“如何判定通过”。本项目建议采用“指标门槛 + 证据文件 + 异常说明”的组合判定方式，以确保结论可执行、可抽查：

- **指标门槛**：本章不直接写死具体数值门槛（避免与实际运行结果冲突），最终由项目组在完成实验后根据课程要求与真实数据填入阈值，并说明依据（例如论文对照、课程建议、或项目内部约定）。  
- **证据文件**：每个结论必须对应到一个可定位的输出文件或截图（轨迹、评测报告、解码汇总、可视化页面记录等），便于答辩抽查与复核。  
- **异常说明**：若存在未达标项，需明确触发条件与影响范围，并给出规避方案与修复计划；对不影响主功能验收的问题，可作为“已知问题”接受，但需标注风险。

建议在最终提交版中补充如下“验收门槛表”，将门槛与证据位置绑定，形成可执行的验收清单：

表 5-0 验收门槛与证据（建议模板）
| 维度 | 指标 | 建议门槛（填写） | 证据文件（填写） | 判定 |
| --- | --- | --- | --- | --- |
| 可用性 | DR/PRR/BER |  |  |  |
| 隐蔽性 | ΔSR/ΔSteps/ΔTime |  |  |  |
| 鲁棒性 | p-恢复曲线/阈值 |  |  |  |
| 可信度 | FPR（无水印/错 key） |  |  |  |

### 5.1.3 关键验收输出（文档与证据）

为满足课程项目书的验收要求，本章输出强调“可审计证据链”，建议最终提交/留存以下材料（可按课程格式取舍）：

- 环境与配置记录：模型、推理参数、任务集/分片、随机种子、payload 设定等（对应 `experiments/*/configs/*` 与 `.env.example` 的约束）。  
- 运行产物归档：轨迹、评测结果、解码结果、汇总分析报告（对应 `output/` 下的结构化文件）。  
- 指标汇总表：面向“可用性/隐蔽性/鲁棒性”的关键指标表格（本章给出统一模板）。  
- 问题闭环清单：异常样例、影响范围、处置策略与复测结果。

---

## 5.2 测试依据、环境与范围说明

### 5.2.1 测试依据（与仓库实现对齐）

本章的验证口径来自项目实现本身，核心依据包括：

- 行为水印算法与同步机制：`agentmark/core/watermark_sampler.py`（差分采样、稳定排序、上下文密钥 DRBG）。  
- 纠删码与恢复：`agentmark/core/rlnc_codec.py` 与 `experiments/rlnc_trajectory/`（擦除鲁棒性、误报率分析）。  
- SDK 封装与结构化输出：`agentmark/sdk/watermarker.py`（bit_index/round 管理、distribution_diff 等前端所需结构）。  
- 网关代理与 API 兼容：`agentmark/proxy/server.py`（OpenAI 兼容 `/v1/chat/completions`、watermark 字段回传、session 维护）。  
- 环境与实验说明：`README_zh.md`、`README.md`、`项目文档/启动指南.md`、`项目文档/水印SDK使用说明.md`。

以上依据保证本章内容“可落地、可复现、可审计”，避免脱离工程实现的泛化描述。

### 5.2.2 硬件与软件环境（记录口径）

本项目在工程上支持两类运行模式：  
（1）远程 API 模式：通过 DeepSeek/OpenAI 兼容接口调用模型，侧重验证“代理 + 行为水印”的落地可行性；  
（2）本地模型模式：ToolBench 本地模式结合 SynthID，侧重验证“行为水印 + 文本水印”复合链路（可选）。

报告体裁下建议以“配置记录表”方式描述环境信息，避免在正文堆叠安装命令。推荐记录项如下（可据实际填写）：

表 5-1 测试环境记录（建议模板）
| 类别 | 项目 | 记录值 | 说明 |
| --- | --- | --- | --- |
| 硬件 | CPU/内存/存储 |  | API 模式对 GPU 依赖较弱 |
| 硬件 | GPU/显存（可选） |  | 本地模型与文本水印验证使用 |
| 软件 | Python 版本 |  | ToolBench/ALFWorld 以 3.9+ 为主 |
| 软件 | Node.js 版本 |  | Dashboard 前端 |
| 软件 | 运行模式 | API/本地 | 与配置文件一致 |
| 外部服务 | 模型/平台 |  | DeepSeek/OpenAI 兼容 |

### 5.2.3 数据与场景范围（覆盖面说明）

本项目的评测场景覆盖三类典型 Agent 环境，分别对应不同类型的“行为序列”：

- **ToolBench**：工具调用型 Agent，行为体现为工具选择与调用序列，适合验证“工具候选集较大、概率分布多峰”的水印嵌入与恢复。  
- **ALFWorld**：具身智能/家庭任务，行为体现为规划与执行动作序列，适合验证“多步规划、状态演进、长序列”的稳定性与效用保持。  
- **Oasis**：社会模拟场景（Twitter/Reddit），行为体现为用户互动与内容生成，适合验证“社会行为分布与跨回合上下文”的可用性与隐蔽性。

除上述主环境外，本项目提供两类“专项鲁棒性验证”：

- **RLNC 擦除/丢包鲁棒性**：`experiments/rlnc_trajectory/`。  
- **语义重写鲁棒性**：`experiments/semantic_rewriting/`。

需要强调的是：行为水印方案对“输入形态”有明确前提。本项目的验证默认满足以下条件，否则无法完成嵌入与恢复：

- 每一步能够构造**候选动作集合**（例如工具列表或候选行为列表）；  
- 每一步能够获得**候选动作的概率/权重分布**（至少可归一化为分布）；  
- 编码与解码使用一致的 **context_for_key**（或可重建的历史窗口）；  
- 轨迹日志能够结构化保存关键字段，支持事后审计与复算。

该约束与仓库的 SDK 说明一致：若接入方只能拿到最终动作文本而无法获得候选集与概率分布，则无法使用本项目的行为层水印方案，需改用其他类型的水印（例如纯文本水印）或先补齐“评分/概率输出”能力。

### 5.2.4 对照与公平性（项目评审常见要求）

为保证“baseline vs watermark”的对比公平，本项目采用以下统一原则（并建议在最终报告中逐项勾选/确认）：

- 同一任务集合：两组使用相同任务 ID、相同初始条件与相同评测规则。  
- 同一模型条件：两组调用同一模型（或同一 checkpoint），推理参数保持一致。  
- 同一随机性策略：种子与并发策略明确记录，避免偶然性掩盖水印影响。  
- 仅改变水印相关变量：除采样/编码模块外，不引入额外策略差异。  
- 结果可审计：保留每一步的概率分布、context_for_key、bit_index 前后等关键证据，支持定位与复查。

### 5.2.5 运行形态与系统边界（部署/接口视角）

从项目交付与验收视角，需要先说明“Agent 原本如何调用模型”，再说明“我们提供什么接口来低成本接入水印能力”。

在本项目的典型使用方式中，外部 Agent 与模型的交互遵循 OpenAI 风格的 Chat Completions 调用范式：Agent 以 `messages`（对话上下文）为输入，在工具调用型场景下同时携带 `tools`（可用工具定义）或等价的候选集合信息；模型返回自然语言内容或 tool-call 结构，驱动 Agent 执行后续动作。也就是说，Agent 的业务逻辑主要依赖“一个稳定的模型调用接口”，而不依赖水印逻辑本身。

在此基础上，AgentMarkWeb 提供一个与上述范式兼容的**代理网关接口**：对外保持请求/响应结构兼容，并在不改变 Agent 决策逻辑的前提下完成“候选分布解析 → 行为水印采样嵌入 → 结构化水印信息回传”。因此接入方通常只需将模型调用地址指向代理服务（或在 SDK 层包装采样过程），即可获得水印能力；额外的水印同步信息（如 `context_for_key`、`session_id`）可通过扩展字段传入，用于提升跨回合一致性与可审计性。

基于上述调用方式与接口设计，本项目的系统边界可概括为：AgentMarkWeb 不要求改造外部 Agent 的业务逻辑，而是通过“网关代理 + SDK/日志规范 + Dashboard 可视化”完成接入与验收支撑。

- **外部 Agent（业务侧）**：保持原策略与业务代码不变，仅调整模型调用出口或接入 SDK 的采样封装。  
- **网关代理（行为水印执行点）**：注入评分指令、解析候选分布、执行差分采样、附加 `watermark` 字段，并维护 session 状态。  
- **离线分析（验收统计）**：对轨迹、评测与解码结果做汇总，产出项目报告中的表格与结论。  
- **Dashboard（演示与抽查）**：对结构化水印输出（如 distribution_diff）做可视化展示，用于演示、人工抽查与快速定位。

这种边界划分的直接收益是“水印能力外置化”：接入成本低、回滚成本低，同时水印逻辑与任务逻辑解耦，使得验收时可以独立审计水印证据而不依赖业务方内部实现细节。

为便于评审理解系统如何在不破坏兼容性的情况下携带水印信息，建议在报告中补充“接口字段约定”：

表 5-1（扩展）关键接口字段约定（建议模板）
| 类型 | 字段/位置 | 用途 | 备注 |
| --- | --- | --- | --- |
| 请求 | `tools` / `candidates` | 候选行为集合输入 | 两种输入形式均可适配 |
| 请求 | `extra_body.agentmark.context` 或 `context` | 水印同步/解码上下文 | 建议包含 task_id/step_id/obs_hash 等 |
| 请求 | `extra_body.agentmark.session_id` 或 header | 跨请求 session 维持 | 多轮对话建议开启 |
| 响应 | `watermark` 扩展字段 | 携带采样/解码/可视化结果 | 原响应结构保留 |

### 5.2.6 交付产物与证据文件结构（便于验收抽查）

为了让验收“可抽查、可复核”，建议在项目交付中明确输出产物的归档规则。AgentMarkWeb 的实验与验证产物主要集中在 `output/` 目录下，并按环境/运行轮次/模式（baseline 或 watermark）组织。不同环境的细节结构略有差异，但建议在报告中说明以下三类证据：

1) **任务轨迹证据**（证明系统真实运行并产生行为序列）  
包括任务输入、逐步动作、候选集与概率分布（或可重建信息）、模型响应、工具调用结果等。该类证据用于解释成功/失败原因，也是水印同步与恢复的必要基础。

2) **评测结果证据**（证明效用与任务完成情况）  
包括任务成功率判定、步骤数与耗时统计、以及与 baseline 的对照结论。ToolBench 的评测结果通常以“是否解决/是否完成”的形式存在，ALFWorld 则以任务成功与步骤上限等形式体现。

3) **水印证据**（证明可嵌入、可恢复、可判定）  
包括 watermark_trace / detection_trace / decoded_bits / 解码汇总等。对于采用 RLNC 的场景，还应能够追溯“收到包数量、矩阵秩、是否满秩”等恢复判定依据。

项目书写作中建议把“证据文件位置”写入总览表（如表 5-10 的“证据/输出位置”列），并在答辩时准备少量典型样例（成功/失败/对抗）用于现场抽查。

---

## 5.3 验证方法与指标口径（项目书表述）

### 5.3.1 指标体系总览

本项目不追求“指标越多越好”，而是围绕三类质量目标建立“最小但完整”的指标体系：

- **可用性**：水印检出率（DR）、有效载荷恢复率（PRR）、比特错误率（BER）、嵌入容量（bits/step、bits/task）。  
- **隐蔽性**：任务成功率差异（ΔSR）、步骤数差异（ΔSteps）、耗时差异（ΔTime）、行为分布差异（如 KL/JS）。  
- **鲁棒性**：不同丢包率下的恢复成功率曲线、恢复阈值（最小接收包/步数）、误报率（FPR）与错误密钥下的一致性失败率。

### 5.3.2 指标定义（用于统一口径）

为避免“同名指标、不同算法口径”的问题，本项目建议统一采用如下定义：

- 水印检出率（Detection Rate, DR）  
  DR = 解码成功样本数 / 总样本数 × 100%。

- 比特错误率（Bit Error Rate, BER）  
  BER = 解码比特串与期望比特串的逐位差异比例（按样本平均，必要时可加权）。

- 有效载荷恢复率（Payload Recovery Rate, PRR）  
  PRR = 成功恢复 payload 的次数 / 总次数 × 100%（适用于 RLNC/纠删码方案）。

- 延迟增量（Latency Overhead）  
  ΔT = T_watermark − T_baseline；相对增量 = (T_watermark / T_baseline − 1) × 100%。

### 5.3.3 证据留存（项目验收更看重“可追溯”）

由于行为水印的验证高度依赖中间量，本项目强调“结构化证据留存”，建议至少保留以下字段（实际字段在 ToolBench/ALFWorld 的轨迹日志中已出现或可映射）：

- 候选行为集合与概率分布（用于复算采样/解码）；  
- 选择动作与 round_num（用于同步）；  
- context_for_key 或可重建的历史窗口（用于生成上下文密钥）；  
- bit_index_before/after 或等价的嵌入位数记录（用于容量统计与 RLNC 包索引映射）；  
- watermark_trace / detection_trace / decoded_bits（用于复核 DR/BER/PRR）。  

该口径与仓库现有实现一致：差分采样在 `agentmark/core/watermark_sampler.py` 中对排序稳定性做了工程加固（stable sort + 概率舍入），RLNC 相关的擦除评测与误报率分析在 `experiments/rlnc_trajectory/` 中提供了直接可复用的统计工具。

### 5.3.4 结果统计口径（项目报告常见写法）

为避免“同一指标不同统计方法导致不可比”，建议在最终报告中明确以下统计口径（不涉及命令行，仅描述规则）：

- **样本单位**：以“任务（task）”作为主要统计单位；在需要时补充“步骤（step）”级别统计用于容量与同步分析。  
- **重复运行**：对于含随机性的场景（温度采样、并发），建议重复运行若干次并给出均值与波动范围；若资源有限，可至少给出一次完整运行并在限制说明中注明。  
- **异常处理**：当某任务出现“概率不可解析/候选集缺失/请求失败”等异常时，需明确该样本在统计中的处理方式（剔除、计为失败、或单独归类），并在备注中说明原因。  
- **证据抽查**：建议每个环境抽查典型轨迹（成功/失败/对抗各至少一条），确保汇总数字与原始证据一致。

项目书体裁下，统计口径的价值在于“让结论可被复核”，而不是追求复杂统计方法。最终报告只需做到：同一口径下对照组与水印组可比、异常样本处理透明、证据文件可定位。

---

## 5.4 功能验证（以“可用性”为主）

### 5.4.1 行为水印链路的功能完整性说明

从工程交付角度，行为水印功能是否“可用”，必须满足三项条件：

1) **可嵌入**：系统能够在“候选行为概率分布”存在的前提下，完成差分采样并将 payload 比特嵌入到动作选择中。  
2) **可记录**：系统能够在每一步留存足够信息，使得事后能复算与追溯嵌入过程（尤其是 context_for_key 与 bit_index 的连续性）。  
3) **可恢复**：系统能够在同等输入条件下完成解码，并输出可审计的恢复结果（bit stream 或 payload）。

上述三项在仓库中分别由以下模块支撑：  
（1）差分采样与解码：`agentmark/core/watermark_sampler.py`；  
（2）状态与结构化输出：`agentmark/sdk/watermarker.py`；  
（3）网关与兼容接口：`agentmark/proxy/server.py`。  

为了体现项目实现的“真实可审计细节”，这里补充说明该链路在工程上依赖的关键机制（强调可复核，而非算法推导）：

- **上下文密钥绑定**：系统以 context_for_key 或历史窗口生成确定性密钥，保证编码与解码的随机性可同步。  
- **稳定排序与概率舍入**：对候选概率分布采用稳定排序，并对概率进行舍入处理，避免浮点噪声导致“等概率项排序变化”而引发同步失败。  
- **bit_index 连续性**：以 bit_index_before/after 记录每一步嵌入消耗的比特数，使容量统计与 RLNC 包索引映射可复算。  
- **结构化 watermark 字段**：代理侧将 action、bits_embedded、decoded_bits、distribution_diff 等结果封装回传，便于前端展示与审计。

### 5.4.2 水印检出能力（Availability）呈现方式

检出能力是可用性验收的核心。考虑到项目报告以表格为主，建议采用“环境 × 任务规模 × 序列长度”的组织方式，将结果汇总成一张主表，并在必要处补充分组统计（如按任务类型、按候选集大小等）。

表 5-2 水印检出能力汇总（建议模板）
| 环境 | 任务数 | 平均步骤 | bits/任务 | DR | BER | PRR（如适用） | 备注 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| ToolBench |  |  |  |  |  |  |  |
| ALFWorld |  |  |  |  |  |  |  |
| Oasis |  |  |  |  |  |  |  |

在结果解释上，建议强调“检出率与序列长度的关系”：序列越长、可嵌入位数越多，DR/PRR 通常越高；当候选集过小或概率高度集中时，嵌入容量可能下降，应在备注中说明原因并给出证据位置（日志/报告编号）。

为更符合项目书表达，可将可用性验证拆成三类验收点，并在表格备注中对应标注：

- **嵌入稳定性**：是否出现“本应嵌入但实际未嵌入”的异常（如候选集解析失败、概率不可解析）。  
- **恢复稳定性**：在同等输入条件下是否出现“可嵌入但不可恢复”的同步问题（context 不一致、候选集变化等）。  
- **证据完整性**：轨迹中是否具备“复算解码”所需字段；缺字段视为证据不完备，应单独归类说明。

同时，不同环境的可用性验证侧重点略有差异，建议在最终报告中分环境补充一句结论说明：

- ToolBench：候选集合规模大，重点关注大候选集下的排序稳定性与嵌入容量表现；  
- ALFWorld：序列更长，重点关注 bit_index 连续性与长链路解码的稳定性；  
- Oasis：上下文跨回合更强，重点关注 context_for_key 设计与 session 维持的可靠性。

### 5.4.3 API 适配与插件兼容性（工程可接入性）

项目交付强调“能接入、少侵入”。本项目的网关代理按 OpenAI 兼容风格提供 `/v1/chat/completions` 接口，响应保持原结构并新增 `watermark` 字段，允许外部 Agent 在不修改业务逻辑的前提下通过更换出口地址实现接入。兼容性验收建议用“矩阵表”呈现。

表 5-3 兼容性验收矩阵（建议模板）
| 模型/平台 | API 协议兼容性 | Tool Calling 兼容性 | candidates 输入 | extra_body 扩展 | watermark 字段回传 | 结论 |
| --- | --- | --- | --- | --- | --- | --- |
| DeepSeek | ✅/⚠️/❌ | ✅/⚠️/❌ | ✅/⚠️/❌ | ✅/⚠️/❌ | ✅/⚠️/❌ |  |
| OpenAI 兼容 | ✅/⚠️/❌ | ✅/⚠️/❌ | ✅/⚠️/❌ | ✅/⚠️/❌ | ✅/⚠️/❌ |  |
| 其他（可选） |  |  |  |  |  |  |

说明：当出现 ⚠️/❌ 时，应在项目问题清单中记录“触发条件、影响范围、临时规避策略、计划修复方案”，并在复测后更新结论。

此外，兼容性验收建议补充“候选集与分布的输入来源”说明，以减少评审误解：

- 对于 tool-calling 型 Agent，可直接从请求 `tools` 参数推导候选动作集合；  
- 对于非 tool-calling 场景，可通过 `candidates` 显式传入候选集合；  
- 对于默认只输出动作文本的模型，需要通过额外提示或评分工具机制让模型产出可解析的权重/概率分布，否则无法进行行为水印嵌入与恢复。

### 5.4.4 端到端闭环（代理 → 任务 → 解码 → 可视化）

除算法正确性外，项目验收更关注“闭环是否完整”。AgentMarkWeb 提供 Dashboard 与结构化输出字段（如 distribution_diff）用于可视化验证，建议将端到端闭环拆为一组验收项，记录是否通过与证据位置。

表 5-4 端到端验收项（建议模板）
| 验收项编号 | 验收项 | 判定标准 | 证据/输出 | 结论 |
| --- | --- | --- | --- | --- |
| E2E-01 | 代理可转发并回传 watermark | 响应兼容 + 附加 watermark 字段 | 会话记录/响应样例 |  |
| E2E-02 | session 连续性与 bit_index 连续 | 同 session 下 bit_index 连续递增 | 日志字段/trace |  |
| E2E-03 | 解码结果可追溯 | 解码所需字段齐全且可复算 | decoded_bits/summary |  |
| E2E-04 | Dashboard 可视化可用 | 分布差异与水印信息可展示 | 页面截图/记录 |  |

为保持项目书风格，可在验收项后补充“闭环说明”：系统交付的是“可验证归属”的闭环能力——不仅能生成水印，还能提供验证所需证据，并在需要时解释“为何可判定/为何不可判定”。在答辩或验收抽查中，建议以少量典型轨迹作为样例，展示从输入到 watermark 输出、再到解码与可视化的完整链路。

### 5.4.5 分环境验证要点（更贴近项目验收叙述）

虽然三类环境的任务形态不同，但从验收角度看，验证要点可以统一为“输入可得、采样可执行、证据可追溯、结果可复核”。建议在报告中按环境补充以下要点（强调结论与证据位置，而非运行过程）：

1) **ToolBench（工具调用型 Agent）**  
验证重点：  
（1）在候选工具集合规模较大时，候选集解析与概率分布稳定性是否满足同步要求；  
（2）水印嵌入是否会显著降低工具调用任务成功率（隐蔽性侧证）；  
（3）轨迹中 `watermark_trace` 等证据是否齐全（bit_index 连续、round 标识清晰）。  
证据建议：预测轨迹、评测结果、解码汇总/分析报告（见 `experiments/toolbench/` 与分析脚本产物）。

2) **ALFWorld（具身智能 Agent）**  
验证重点：  
（1）长序列多步交互下，bit_index 与 round_num 是否稳定递进；  
（2）失败任务是否能定位到具体 step 的候选集/分布异常或环境约束；  
（3）`evaluation_report` 与 `detection_trace` 是否足以支撑事后解码与复核。  
证据建议：评测报告 JSON、解码结果 JSON、聚合分析表（见 `experiments/alfworld/` 的报告与分析脚本）。

3) **Oasis（社会模拟 Agent）**  
验证重点：  
（1）跨回合上下文强依赖场景下，context_for_key 的设计是否稳定（不因无关文本波动导致同步破坏）；  
（2）水印机制是否影响用户行为/内容生成的总体分布（隐蔽性）；  
（3）输出证据是否可用于“归属证明”展示（必要时配合 Dashboard 展示）。  
证据建议：实验日志、统计指标、典型样例截图（见 `experiments/oasis_watermark/`）。

为便于项目评审“一眼看懂覆盖面”，可增加一张汇总表：

表 5-4（扩展）分环境验证覆盖（建议模板）
| 环境 | 主要验证点 | 关键证据 | 备注 |
| --- | --- | --- | --- |
| ToolBench | 工具候选集稳定性、DR/ΔSR、watermark_trace 完整性 | 轨迹/评测/解码汇总 |  |
| ALFWorld | 长序列同步、报告可复核、失败定位 | evaluation_report/detection_trace |  |
| Oasis | 上下文稳定、分布影响、展示可用 | 日志/统计/截图 |  |

---

## 5.5 性能评估（以“工程代价可控”为主）

### 5.5.1 响应延迟与吞吐影响

性能验收关注“额外代价是否可接受”。由于水印机制在推理流程中增加概率解析、分布重组与采样，因此应评估其对整体响应时间的影响，并区分“模型端耗时”与“代理端耗时”，避免网络抖动干扰判断。

表 5-5 延迟与效率对比（建议模板）
| 环境 | 指标 | baseline | watermark | 增量 | 备注 |
| --- | --- | --- | --- | --- | --- |
| ToolBench | 平均耗时 |  |  |  |  |
| ToolBench | P90 耗时（可选） |  |  |  |  |
| ALFWorld | 平均耗时 |  |  |  |  |
| Oasis | 平均耗时 |  |  |  |  |

项目报告建议采用“均值 + 波动范围”的写法（例如标准差/分位数），重点说明：增量主要来自代理处理还是来自调用链路；若增量集中在代理侧，则属于可优化项，可进入后续问题闭环。

在“工程代价”表述上，建议同时给出两类解释维度，帮助评审理解增量来源：

- **算法侧代价**：排序、分布重组与采样的复杂度随候选集规模增长；候选集越大，代理侧处理越敏感。  
- **工程侧代价**：日志序列化、落盘、网络传输等非算法因素可能成为主要瓶颈，尤其在并发与长会话下更明显。

因此，性能评估结论应尽量配套“瓶颈解释”，避免出现“慢了但原因不明”的不可复用结论。

### 5.5.2 资源占用与部署可行性

资源占用评估的目标是确认系统在典型部署形态下能够长期稳定运行。建议按“模块 × 场景（单会话/并发）”记录内存与 CPU 占用，并以“峰值/稳态”区分。

表 5-6 资源占用统计（建议模板）
| 模块 | 场景 | baseline 内存/CPU | watermark 内存/CPU | 峰值差异 | 备注 |
| --- | --- | --- | --- | --- | --- |
| Proxy | 单会话 |  |  |  |  |
| Proxy | 并发 N |  |  |  |  |
| Dashboard 后端 | 单会话 |  |  |  |  |
| Dashboard 前端 | 单会话 |  |  |  |  |

资源占用评估还应说明“可部署性边界”，例如：

- 代理侧 session 缓存上限与淘汰策略对内存的影响；  
- 并发情况下日志写入与锁竞争对 CPU 的影响；  
- 长会话/大候选集场景下，前端可视化渲染压力与卡顿风险。

这类内容属于工程常见瓶颈点，项目书体裁建议以“风险提示 + 处置建议”呈现，不必展开过度学术推导。

### 5.5.3 可扩展性（并行与规模化）

当任务规模扩大时，需要关注两类风险：  
（1）并行执行下的会话隔离与日志一致性；  
（2）解码与统计阶段的性能瓶颈（例如 RLNC 解码的矩阵求解成本随 payload 增大而上升）。

建议在项目报告中至少给出“并发规模变化”下的两项指标：吞吐量（或单位时间完成任务数）与解码成功率，说明系统在规模化情况下仍能保持质量目标。

为增强可读性，可增加一张“并发规模影响表”用于总结趋势（不必强制填满，可按实际测量项填写）：

表 5-6（扩展）并发规模影响（建议模板）
| 并发规模 | 吞吐量（任务/小时） | 平均耗时 | DR/PRR | 异常率 | 备注 |
| --- | --- | --- | --- | --- | --- |
| 1 |  |  |  |  |  |
| N |  |  |  |  |  |

### 5.5.4 性能与隐蔽性的联动说明（项目结论写法）

在行为水印系统中，“性能指标”往往是隐蔽性的侧证：若水印采样显著改变动作选择，通常会带来成功率下降、步骤数增加或耗时上升。因此项目报告建议把以下四类指标进行联动解释，形成完整工程结论链：

- 成功率（SR）：若 watermark 组显著低于 baseline，应结合具体失败样例分析是否存在采样偏置或候选集异常。  
- 步骤数（Steps）：若 watermark 组步骤显著增加，通常意味着动作选择发生偏移，需要回查分布差异与关键步骤。  
- 耗时（Time）：若耗时差异显著但成功率差异不大，可能主要来自代理解析/日志写入的工程开销，可作为优化方向。  
- 分布差异（KL/JS 等）：用于解释“是否真的改变了行为分布”，并与成功率/步骤数相互印证。

这种联动写法更贴近项目评审预期：不仅给出数字对比，还能给出“为什么会这样”的工程解释，并能落到可追溯证据。

---

## 5.6 安全性与鲁棒性评估（以“干扰下仍可判定”为主）

### 5.6.1 丢包/擦除场景恢复能力（RLNC）

真实部署中常见干扰包括：网络波动导致请求失败、浏览器插件中断导致步骤缺失、日志裁剪导致轨迹不完整等。对此，本项目引入 RLNC（随机线性网络编码）以增强擦除鲁棒性，并在 `experiments/rlnc_trajectory/` 提供了成套评估工具与配置。

项目报告中建议以“丢包率 p → 恢复能力”的方式呈现，突出系统在不同干扰强度下的可恢复性边界：

表 5-7 擦除鲁棒性结果（建议模板）
| 场景 | 丢包率 p | 恢复成功率 | 平均接收步数 | 平均接收包数 | 备注 |
| --- | --- | --- | --- | --- | --- |
| ToolBench（RLNC） |  |  |  |  |  |

从项目评审的表达方式看，这里需要突出一个工程结论：RLNC 的价值并非“把检出率提高一点点”，而是让系统在“日志不完整”的现实条件下仍能给出可靠判定。建议在最终报告中明确写出“恢复阈值”：在 payload 长度与冗余配置固定时，大致需要多少有效包/步骤才能达到稳定恢复。

### 5.6.2 误报率与错误密钥（可信度要求）

行为水印用于版权与归属证明时，误报率是工程可信度的底线指标。本项目提供“无水印/错误 key”条件下的误报率评估思路（见 `experiments/rlnc_trajectory/scripts/analyze_fpr.py`），用于回答：在没有植入水印或密钥不一致的情况下，系统是否会“误判为存在水印”。

表 5-8 误报率统计（建议模板）
| 条件 | payload 长度 n | 冗余/阈值设置 | 试验次数 | FPR | 备注 |
| --- | --- | --- | --- | --- | --- |
| 无水印（随机） |  |  |  |  |  |
| 错误 key |  |  |  |  |  |

误报率部分建议补充“接受规则”文字说明，避免不同人用不同规则复算导致争议。例如：是否要求“满秩 + 一致可解”，是否允许部分恢复即判定为存在水印等。项目上应优先采用严格规则：宁可给出“证据不足不可判定”，也不应牺牲可信度换取更高的检出率。

### 5.6.3 轨迹截断与日志缺失（工程常见异常）

除随机丢失外，“只保留轨迹前缀/后缀”是更贴近工程现实的异常类型：例如任务中途失败退出只留下前半段轨迹。项目报告建议在结论中说明两点：  
（1）在固定 payload 与参数设置下，系统需要多长的有效轨迹才具备高概率恢复；  
（2）当轨迹长度不足时，系统应给出“无法判定/证据不足”的工程结论，而非输出不可信结果。

建议在报告中补充一句说明：轨迹缺失不仅影响水印恢复，也可能影响效用统计（任务未完成导致成功率与耗时不具可比性）。因此在“缺失场景”下，隐蔽性指标（成功率/耗时）与可用性指标（DR/PRR）应分别统计，不应混在同一口径里。

### 5.6.4 语义重写攻击（分布稳定性与同步性）

语义重写是现实中可能出现的对抗：攻击者不改动作集合，但改写观测/上下文，间接影响模型概率分布。项目提供了语义重写的鲁棒性测试方案（`experiments/semantic_rewriting/`），项目报告中建议用“改写前后分布差异 + 解码成功率”联合呈现，以说明水印同步机制对上下文扰动的敏感度与边界。

表 5-9 语义重写鲁棒性（建议模板）
| 任务集/样本 | 改写强度（描述） | 分布差异（KL/JS） | 解码成功率 | 备注 |
| --- | --- | --- | --- | --- |
|  |  |  |  |  |

语义重写部分建议在最终报告中增加少量“典型案例小结”：指出哪些类型的观测改写对概率分布影响更大、哪些情况下解码会失败，以及失败是否可以通过改进 context_for_key 的设计或增加纠删码冗余来缓解。

### 5.6.5 同步与上下文一致性（可解释性要求）

行为水印的一个工程难点是“同步”：编码与解码必须基于一致的候选集、概率分布与 context。项目在实现层面通过稳定排序与概率舍入降低浮点噪声导致的排序漂移风险；在报告层面，建议明确当解码失败时的判定与定位方式：失败发生在哪一步、候选集是否变化、context 是否缺失、bit_index 是否断裂等。此类“可解释失败”同样属于工程质量的一部分。

### 5.6.6 网络波动与服务异常（工程可恢复性）

在真实部署中，模型服务与网络链路的不稳定是常态：包括超时、重试、间歇性失败、返回内容不完整等。对于行为水印系统而言，这类异常会被体现为“步骤级别的缺失或不可解析”，本项目建议在报告中说明两点：

1) **异常对水印的影响边界**  
当部分步骤缺失时，水印恢复能力主要取决于剩余有效步骤携带的比特量与纠删码冗余（对应 5.6.1 的恢复曲线）。当缺失导致有效信息不足时，系统应倾向输出“证据不足不可判定”，而非输出低可信结论。

2) **异常对业务的影响边界**  
网络异常首先影响任务本身的完成情况，因此隐蔽性评估必须区分“水印造成的效用变化”与“网络抖动造成的效用变化”。项目报告建议在测试记录中保留失败原因分类（网络失败/解析失败/环境失败/策略失败），并在指标汇总中注明异常占比。

在工程处置上，推荐原则是“可恢复、可审计、可降级”：保证系统在异常发生时不崩溃、证据可追溯、必要时可降级为不嵌入/不判定并明确记录。

---

## 5.7 结果汇总、问题闭环与改进方向

### 5.7.1 核心指标汇总表（面向验收）

项目报告建议提供一张“总览表”作为本章的最终落点，用于一眼判断是否满足质量目标。该表不要求列出所有细节，但要求口径统一、可追溯到证据文件。

表 5-10 核心指标总览（建议模板）
| 维度 | 指标 | ToolBench | ALFWorld | Oasis | 证据/输出位置 | 结论 |
| --- | --- | --- | --- | --- | --- | --- |
| 可用性 | DR / BER / PRR |  |  |  |  |  |
| 隐蔽性 | ΔSR / ΔSteps / ΔTime |  |  |  |  |  |
| 鲁棒性 | 丢包恢复成功率曲线 |  |  |  |  |  |
| 可信度 | FPR（无水印/错 key） |  |  |  |  |  |

### 5.7.2 典型问题与处置策略（建议模板）

项目评审通常要求“发现问题并闭环”。建议将问题按“触发条件—影响—处置—复测”记录：

表 5-11 问题闭环清单（建议模板）
| 编号 | 问题描述 | 触发条件 | 影响范围 | 临时规避 | 修复计划 | 复测结论 |
| --- | --- | --- | --- | --- | --- | --- |
|  | 概率不可解析导致无法嵌入 |  |  |  |  |  |
|  | context 丢失导致解码失败 |  |  |  |  |  |
|  | 高丢包率下秩不足 |  |  |  |  |  |

### 5.7.3 验收流程建议（项目管理视角）

为配合课程项目书或答辩评审，建议采用“先闭环、后指标、再对抗”的验收流程，使评审关注点从“如何运行脚本”转向“是否具备可交付能力”：

1) **闭环验收**：先确认代理/SDK/解码/可视化链路可跑通，能够输出结构化证据；  
2) **指标验收**：在固定环境与任务集合下填充 DR/ΔSR/丢包恢复等核心指标表，形成可核对的结果；  
3) **异常抽查**：抽查失败样本，确认失败原因可解释且证据完整；  
4) **问题闭环**：对已知问题给出规避/修复/复测结论，形成可交付状态。

该流程有助于把“科研式指标追逐”转为“工程式可交付验收”，更贴合项目书的表达与评审预期。

### 5.7.4 改进方向（基于本项目实现的可行项）

结合现有代码结构与工程约束，项目后续可落地的改进方向包括：

- **降低代理侧开销**：优化概率解析与日志写入路径，减少延迟增量与资源占用波动。  
- **增强输入鲁棒性**：对“候选集缺失、概率全等、概率噪声较大”等边界输入提供更明确的降级策略与可审计提示。  
- **统一证据格式**：进一步规范不同环境下的 trace 字段命名与结构，降低跨环境汇总分析成本。  
- **提高对抗解释能力**：在语义重写/轨迹截断等对抗下增加失败原因分类统计，形成更清晰的工程边界说明。

---

## 附：材料索引（便于审计与复现）

本节不列出命令行，仅给出“材料路径索引”，便于项目验收时快速定位证据与实现依据：

- 行为水印核心：`agentmark/core/watermark_sampler.py`  
- RLNC 编码解码：`agentmark/core/rlnc_codec.py`  
- SDK 封装：`agentmark/sdk/watermarker.py`  
- 代理网关：`agentmark/proxy/server.py`  
- ToolBench 实验与分析：`experiments/toolbench/`、`experiments/toolbench/scripts/analysis/generate_detailed_report.py`  
- ALFWorld 实验与分析：`experiments/alfworld/`、`experiments/alfworld/scripts/analyze_results.py`、`experiments/alfworld/scripts/decode_report.py`  
- RLNC 鲁棒性与 FPR：`experiments/rlnc_trajectory/`  
- 语义重写鲁棒性：`experiments/semantic_rewriting/`  
- 运行与使用说明：`README_zh.md`、`项目文档/启动指南.md`、`项目文档/水印SDK使用说明.md`
